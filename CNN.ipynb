{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- [Brain Tumor Detection Using Convolutional Neural Networks](https://medium.com/@mohamedalihabib7/brain-tumor-detection-using-convolutional-neural-networks-30ccef6612b0)\n",
        "- [Build an Image Classification Model using Convolutional Neural Networks in PyTorch](https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/)\n",
        "\n",
        "Due to the limitations in computation, this CNN model is a simple CNN model with a few layers."
      ],
      "metadata": {
        "id": "oLuu4teT_S3V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAD44grpxSDU",
        "outputId": "1f8c0d9e-db46-4bba-d7ce-08399adf7549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLNUg2HBxbjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.transforms import RandomAffine, ToTensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0QbsElnQyF1"
      },
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ3-SD2EQw7_"
      },
      "outputs": [],
      "source": [
        "# Define data transformations (resize, normalize, etc.)\n",
        "resize_transform = transforms.Resize((224, 224))\n",
        "validation_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "\n",
        "# Define the dataset path (the directory containing \"benign\" and \"malignant\" subfolders)\n",
        "# dataset_path = '/content/drive/MyDrive/NUS/Y4S1/IT1244/images'\n",
        "dataset_path = '/content/drive/MyDrive/IT1244/images'\n",
        "\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=resize_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zLzK-SFQ4KG"
      },
      "source": [
        "### Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbd2rHkZQ3KX"
      },
      "outputs": [],
      "source": [
        "class Augmentation:\n",
        "  \"\"\"\n",
        "  Custom image augmentation class for applying multiple transformations to input images.\n",
        "\n",
        "  Parameters:\n",
        "  - num_augmentations (int): The number of augmentations to apply to each input image.\n",
        "  - translation (tuple of floats): Maximum absolute fraction for horizontal and vertical translations.\n",
        "  - shear (tuple of floats): Range of shearing angles (degrees).\n",
        "  - rotation (tuple of floats): Range of clockwise rotation angles (degrees).\n",
        "\n",
        "  Methods:\n",
        "  - __call__(self, img_label): Apply augmentations to an input image-label pair.\n",
        "\n",
        "  Usage:\n",
        "  Instantiate the class and call it with an image-label pair to generate augmented images.\n",
        "\n",
        "  Returns:\n",
        "  - augmentations (list): A list of augmented image-label pairs. Each pair consists of an augmented image (torch.Tensor) and the original label.\n",
        "\n",
        "  Example:\n",
        "  augmentation = Augmentation()\n",
        "  augmented_images = augmentation((input_image, input_label))\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.num_augmentations = 10\n",
        "    self.translation = (15 / 224, 15 / 224)\n",
        "    self.shear = (-15, 15)\n",
        "    self.rotation = (-25, 25)\n",
        "    self.toTensor = transforms.ToTensor()\n",
        "    self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  def __call__(self, img_label):\n",
        "    img, label = img_label\n",
        "    augmentations = []\n",
        "    for _ in range(self.num_augmentations):\n",
        "      aug_transform = RandomAffine(degrees=self.rotation, translate=self.translation, shear=self.shear)\n",
        "      new_img = aug_transform(img)\n",
        "      new_img = self.toTensor(new_img)\n",
        "      new_img = self.normalize(new_img)\n",
        "      augmentations.append((new_img, label))\n",
        "\n",
        "    return augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ6Y76kPRZ2u"
      },
      "source": [
        "### Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCaBjNEZxcKI"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, num_classes=2):\n",
        "    \"\"\"\n",
        "    Initialize a CNN model for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "    - num_classes (int): The number of output classes, typically 2 for binary classification.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(16), # batch normalize to speed up computation\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    )\n",
        "\n",
        "    self.flat = nn.Flatten()\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(in_features=64 * 28 * 28, out_features=num_classes, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of the CNN model.\n",
        "\n",
        "    Parameters:\n",
        "    - x (torch.Tensor): Input data (images).\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Output predictions after passing through the model.\n",
        "    \"\"\"\n",
        "    x = self.cnn_layers(x)\n",
        "    x = self.flat(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RdW-PryRkAc"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "  \"\"\"\n",
        "  Train a neural network model on a training dataset.\n",
        "\n",
        "  Parameters:\n",
        "  - model (torch.nn.Module): The neural network model to be trained.\n",
        "  - train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
        "  - criterion (torch.nn.Module): The loss function used for optimization.\n",
        "  - optimizer (torch.optim.Optimizer): The optimizer for updating model weights.\n",
        "  - device (torch.device): The device (CPU or GPU) on which the training will be performed.\n",
        "\n",
        "  Returns:\n",
        "  - float: The total loss over the training dataset.\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for imgs, targets in train_loader:\n",
        "    imgs, targets = imgs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(imgs)\n",
        "    loss = criterion(outputs, nn.functional.one_hot(targets).float())\n",
        "    # loss = criterion(outputs, targets) # gives mean loss, since reduction = 'mean' for default\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item() * imgs.size(0) # mean loss * batch size = total loss of batch\n",
        "\n",
        "  return running_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5F4O7qbRmWy"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "  \"\"\"\n",
        "  Validate a neural network model on a validation dataset.\n",
        "\n",
        "  Parameters:\n",
        "  - model (torch.nn.Module): The neural network model to be validated.\n",
        "  - val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
        "  - criterion (torch.nn.Module): The loss function used for evaluation (usually the same as used during training).\n",
        "  - optimizer (torch.optim.Optimizer): The optimizer (not used during validation but required as a parameter).\n",
        "  - device (torch.device): The device (CPU or GPU) on which the validation will be performed.\n",
        "\n",
        "  Returns:\n",
        "  - List: A list of predicted probablities for label 1, a list of predicted labels, list of true labels for the validation dataset.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  y_pred_probas = []\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  with torch.no_grad():\n",
        "    for imgs, targets in val_loader:\n",
        "      imgs, targets = imgs.to(device), targets.to(device)\n",
        "      outputs = model(imgs)\n",
        "      proba = F.sigmoid(outputs)[:, 1]\n",
        "      _, pred = torch.max(outputs, dim=1)\n",
        "      y_pred_probas.extend(proba.cpu())\n",
        "      y_pred.extend(pred.cpu())\n",
        "      y_true.extend(targets.cpu())\n",
        "  return y_pred_probas, y_pred, y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVkVJpY3keNV"
      },
      "outputs": [],
      "source": [
        "def calculate_evaluation_metrics(y_true, y_pred, y_pred_probas):\n",
        "  \"\"\"\n",
        "  Calculates the evaluation metrics for validation.\n",
        "\n",
        "  Parameters:\n",
        "  - y_true (List): The true labels of the validation dataset.\n",
        "  - y_pred (List): The predicted labels of the validation dataset, value can be 0 or 1.\n",
        "  - y_pred_probas (List): The predicted probabilities for label 1 of the validation dataset, values are between 0 and 1.\n",
        "\n",
        "  Returns:\n",
        "  - List: A list of float values corresponding to the metrics accuracy, precision, recall, f1, specificity, roc_auc, pr_auc\n",
        "  \"\"\"\n",
        "  roc_auc = roc_auc_score(y_true, y_pred_probas)\n",
        "  precision_n, recall_n, _ = precision_recall_curve(y_true, y_pred_probas)\n",
        "  pr_auc = auc(recall_n, precision_n)\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  specificity = tn / (tn + fp)\n",
        "  return accuracy, precision, recall, f1, specificity, roc_auc, pr_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJEVluuZke57"
      },
      "source": [
        "### CV with no Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsKyuHxeR1SJ"
      },
      "outputs": [],
      "source": [
        "def print_validation_performance(fold_accuracies, fold_precisions, fold_recalls, fold_specificities, fold_f1_scores, fold_roc_auc, fold_pr_auc):\n",
        "  \"\"\"\n",
        "  Calculates and prints the validation performace.\n",
        "\n",
        "  Parameters:\n",
        "  - fold_accuracies (List): The accuracy score obtained from each fold.\n",
        "  - fold_precisions (List): The precision score obtained from each fold.\n",
        "  - fold_recalls (List): The recall score obtained from each fold.\n",
        "  - fold_specificities (List): The specificity score obtained from each fold.\n",
        "  - fold_f1_scores (List): The f1 score obtained from each fold.\n",
        "  - fold_roc_auc (List): The roc_auc score obtained from each fold.\n",
        "  - fold_pr_auc (List): The pr_auc score obtained from each fold.\n",
        "  \"\"\"\n",
        "  mean_accuracy = np.mean(fold_accuracies)\n",
        "  mean_precision = np.mean(fold_precisions)\n",
        "  mean_recall = np.mean(fold_recalls)\n",
        "  mean_specificity = np.mean(fold_specificities)\n",
        "  mean_f1 = np.mean(fold_f1_scores)\n",
        "  mean_roc_auc = np.mean(fold_roc_auc)\n",
        "  mean_pr_auc = np.mean(fold_pr_auc)\n",
        "  print(f\"Mean Accuracy: {mean_accuracy:.2f}\")\n",
        "  print(f\"Mean Precision: {mean_precision:.2f}\")\n",
        "  print(f\"Mean Recall: {mean_recall:.2f}\")\n",
        "  print(f\"Mean Specificity: {mean_specificity:.2f}\")\n",
        "  print(f\"Mean F1 Score: {mean_f1:.2f}\")\n",
        "  print(f\"Mean ROC AUC: {mean_roc_auc:.2f}\")\n",
        "  print(f\"Mean PR AUC: {mean_pr_auc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW0esPcjR3I2",
        "outputId": "4ff6f561-4b77-4b60-bcff-34ab615a9bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Training Fold 1/5\n",
            "initial training dataset size: 184, post-transformation training dataset size: 1840, validation dataset size: 46\n",
            "Epoch 1/20, Loss: 1.4762391251066456\n",
            "Epoch 2/20, Loss: 0.710885253937348\n",
            "Epoch 3/20, Loss: 0.4693754337404085\n",
            "Epoch 4/20, Loss: 0.3190202355384827\n",
            "Epoch 5/20, Loss: 0.4319957797941954\n",
            "Epoch 6/20, Loss: 0.281138444011626\n",
            "Epoch 7/20, Loss: 0.1616867119203443\n",
            "Epoch 8/20, Loss: 0.13250214146531147\n",
            "Epoch 9/20, Loss: 0.08850141000002623\n",
            "Epoch 10/20, Loss: 0.10916643856579195\n",
            "Epoch 11/20, Loss: 0.09241064887331879\n",
            "Epoch 12/20, Loss: 0.0499498567989339\n",
            "Epoch 13/20, Loss: 0.02818650908606208\n",
            "Epoch 14/20, Loss: 0.04147751129272839\n",
            "Epoch 15/20, Loss: 0.2851339775664003\n",
            "Epoch 16/20, Loss: 0.07182457245805342\n",
            "Epoch 17/20, Loss: 0.05120314734825945\n",
            "Epoch 18/20, Loss: 0.010833600513718051\n",
            "Epoch 19/20, Loss: 0.009176806433369284\n",
            "Epoch 20/20, Loss: 0.0041595737206871096\n",
            "Validation Accuracy (Fold 1/5): 0.80\n",
            "Precision (Fold 1/5): 0.84\n",
            "Recall (Fold 1/5): 0.87\n",
            "Specificity (Fold 1/5): 0.69\n",
            "F1 Score (Fold 1/5): 0.85\n",
            "ROC AUC (Fold 1/5): 0.91\n",
            "PR AUC (Fold 1/5): 0.95\n",
            "Training Fold 2/5\n",
            "initial training dataset size: 184, post-transformation training dataset size: 1840, validation dataset size: 46\n",
            "Epoch 1/20, Loss: 1.8975458181422689\n",
            "Epoch 2/20, Loss: 0.6999881664048071\n",
            "Epoch 3/20, Loss: 0.7872501518415368\n",
            "Epoch 4/20, Loss: 0.8530073440593222\n",
            "Epoch 5/20, Loss: 0.5288298499325047\n",
            "Epoch 6/20, Loss: 0.27892109382411706\n",
            "Epoch 7/20, Loss: 0.2989542107219281\n",
            "Epoch 8/20, Loss: 0.2061967494695083\n",
            "Epoch 9/20, Loss: 0.16549826206072518\n",
            "Epoch 10/20, Loss: 0.15516204584551893\n",
            "Epoch 11/20, Loss: 0.12626338280413463\n",
            "Epoch 12/20, Loss: 0.12101166306630425\n",
            "Epoch 13/20, Loss: 0.08577405726132185\n",
            "Epoch 14/20, Loss: 0.09018541142020536\n",
            "Epoch 15/20, Loss: 0.051445813833371455\n",
            "Epoch 16/20, Loss: 0.12810261009827903\n",
            "Epoch 17/20, Loss: 0.21266135813101478\n",
            "Epoch 18/20, Loss: 0.03656538029646744\n",
            "Epoch 19/20, Loss: 0.03403566729925249\n",
            "Epoch 20/20, Loss: 0.01871645103978074\n",
            "Validation Accuracy (Fold 2/5): 0.83\n",
            "Precision (Fold 2/5): 0.87\n",
            "Recall (Fold 2/5): 0.87\n",
            "Specificity (Fold 2/5): 0.75\n",
            "F1 Score (Fold 2/5): 0.87\n",
            "ROC AUC (Fold 2/5): 0.89\n",
            "PR AUC (Fold 2/5): 0.93\n",
            "Training Fold 3/5\n",
            "initial training dataset size: 184, post-transformation training dataset size: 1840, validation dataset size: 46\n",
            "Epoch 1/20, Loss: 1.677778627043185\n",
            "Epoch 2/20, Loss: 0.560209454142529\n",
            "Epoch 3/20, Loss: 0.4372584183578906\n",
            "Epoch 4/20, Loss: 0.5155043472414431\n",
            "Epoch 5/20, Loss: 0.6463233921838843\n",
            "Epoch 6/20, Loss: 0.31204826060844504\n",
            "Epoch 7/20, Loss: 0.20564982230248657\n",
            "Epoch 8/20, Loss: 0.14330026944694313\n",
            "Epoch 9/20, Loss: 0.1264009335766668\n",
            "Epoch 10/20, Loss: 0.12467173357372699\n",
            "Epoch 11/20, Loss: 0.14003416241511055\n",
            "Epoch 12/20, Loss: 0.06193325737410266\n",
            "Epoch 13/20, Loss: 0.11751882181219432\n",
            "Epoch 14/20, Loss: 0.04082793062147887\n",
            "Epoch 15/20, Loss: 0.04204633201108031\n",
            "Epoch 16/20, Loss: 0.07564529229117477\n",
            "Epoch 17/20, Loss: 0.032169374191890594\n",
            "Epoch 18/20, Loss: 0.03213360376413102\n",
            "Epoch 19/20, Loss: 0.03154688917667321\n",
            "Epoch 20/20, Loss: 0.00765304387871014\n",
            "Validation Accuracy (Fold 3/5): 0.85\n",
            "Precision (Fold 3/5): 0.85\n",
            "Recall (Fold 3/5): 0.94\n",
            "Specificity (Fold 3/5): 0.67\n",
            "F1 Score (Fold 3/5): 0.89\n",
            "ROC AUC (Fold 3/5): 0.87\n",
            "PR AUC (Fold 3/5): 0.88\n",
            "Training Fold 4/5\n",
            "initial training dataset size: 184, post-transformation training dataset size: 1840, validation dataset size: 46\n",
            "Epoch 1/20, Loss: 1.9299838252689527\n",
            "Epoch 2/20, Loss: 0.601456850507985\n",
            "Epoch 3/20, Loss: 0.5612162818079409\n",
            "Epoch 4/20, Loss: 0.5019342344740163\n",
            "Epoch 5/20, Loss: 0.26810685085213704\n",
            "Epoch 6/20, Loss: 0.239519799273947\n",
            "Epoch 7/20, Loss: 0.2017944727902827\n",
            "Epoch 8/20, Loss: 0.19098940818854002\n",
            "Epoch 9/20, Loss: 0.1314457015984732\n",
            "Epoch 10/20, Loss: 0.10910321525905443\n",
            "Epoch 11/20, Loss: 0.1650354693279318\n",
            "Epoch 12/20, Loss: 0.16058715250667022\n",
            "Epoch 13/20, Loss: 0.055980850820956024\n",
            "Epoch 14/20, Loss: 0.042415508708876115\n",
            "Epoch 15/20, Loss: 0.028398003542552824\n",
            "Epoch 16/20, Loss: 0.013195235598022523\n",
            "Epoch 17/20, Loss: 0.03581661975983044\n",
            "Epoch 18/20, Loss: 0.04157940055770071\n",
            "Epoch 19/20, Loss: 0.0348578911193687\n",
            "Epoch 20/20, Loss: 0.007175728935829323\n",
            "Validation Accuracy (Fold 4/5): 0.74\n",
            "Precision (Fold 4/5): 0.85\n",
            "Recall (Fold 4/5): 0.74\n",
            "Specificity (Fold 4/5): 0.73\n",
            "F1 Score (Fold 4/5): 0.79\n",
            "ROC AUC (Fold 4/5): 0.88\n",
            "PR AUC (Fold 4/5): 0.94\n",
            "Training Fold 5/5\n",
            "initial training dataset size: 184, post-transformation training dataset size: 1840, validation dataset size: 46\n",
            "Epoch 1/20, Loss: 2.4970314593418785\n",
            "Epoch 2/20, Loss: 0.7364004977371381\n",
            "Epoch 3/20, Loss: 0.5877625272325847\n",
            "Epoch 4/20, Loss: 0.355644565626331\n",
            "Epoch 5/20, Loss: 0.2988977423180705\n",
            "Epoch 6/20, Loss: 0.3070502542283224\n",
            "Epoch 7/20, Loss: 0.24444957880870155\n",
            "Epoch 8/20, Loss: 0.3295454063817211\n",
            "Epoch 9/20, Loss: 0.1402268532987522\n",
            "Epoch 10/20, Loss: 0.18897024237591287\n",
            "Epoch 11/20, Loss: 0.11925618243606194\n",
            "Epoch 12/20, Loss: 0.13817626153645307\n",
            "Epoch 13/20, Loss: 0.03459613209023424\n",
            "Epoch 14/20, Loss: 0.04249298842218907\n",
            "Epoch 15/20, Loss: 0.08674704526429591\n",
            "Epoch 16/20, Loss: 0.024724359555250923\n",
            "Epoch 17/20, Loss: 0.08164062632004852\n",
            "Epoch 18/20, Loss: 0.22243852360255045\n",
            "Epoch 19/20, Loss: 0.10042822186875602\n",
            "Epoch 20/20, Loss: 0.1270644682419041\n",
            "Validation Accuracy (Fold 5/5): 0.78\n",
            "Precision (Fold 5/5): 0.80\n",
            "Recall (Fold 5/5): 0.90\n",
            "Specificity (Fold 5/5): 0.53\n",
            "F1 Score (Fold 5/5): 0.85\n",
            "ROC AUC (Fold 5/5): 0.75\n",
            "PR AUC (Fold 5/5): 0.80\n"
          ]
        }
      ],
      "source": [
        "# set hyperparameters to be used\n",
        "num_epochs = 20\n",
        "batch_size = 32 # can try 16 or 64 also\n",
        "learning_rate = 0.001 # lr between 0.0001 and 0.01\n",
        "\n",
        "# Define cross-validation\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # set device to cuda if using T4 GPU\n",
        "print(device)\n",
        "\n",
        "data = [item[0] for item in dataset.samples]\n",
        "data_labels = [item[1] for item in dataset.samples]\n",
        "\n",
        "augmentation = Augmentation()\n",
        "\n",
        "# Initialize lists to store metrics for each fold\n",
        "fold_accuracies = []\n",
        "fold_precisions = []\n",
        "fold_recalls = []\n",
        "fold_specificities = []\n",
        "fold_f1_scores = []\n",
        "fold_roc_auc = []\n",
        "fold_pr_auc = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(data, data_labels)):\n",
        "  print(f\"Training Fold {fold + 1}/{k}\")\n",
        "\n",
        "  # Split the dataset into training and validation sets for this fold\n",
        "  train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
        "  val_dataset = torch.utils.data.Subset(dataset, val_index)\n",
        "\n",
        "  class_counts = dict(Counter(dataset.targets[i] for i in train_index) )\n",
        "\n",
        "  augmented_images = []\n",
        "  for img_label in train_dataset:\n",
        "    augmented_images.extend(augmentation(img_label))\n",
        "\n",
        "  transformed_val_dataset = []\n",
        "  for img_label in val_dataset:\n",
        "    img, label = img_label\n",
        "    transformed_img = validation_transform(img)\n",
        "    transformed_val_dataset.append((transformed_img, label))\n",
        "\n",
        "  # Create data loaders for training and validation\n",
        "  train_loader = DataLoader(augmented_images, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_loader = DataLoader(transformed_val_dataset, batch_size=batch_size, num_workers=2)\n",
        "  print(f\"initial training dataset size: {len(train_dataset)}, post-transformation training dataset size: {len(augmented_images)}, validation dataset size: {len(transformed_val_dataset)}\")\n",
        "\n",
        "  # Initialize the model\n",
        "  model = CNN()\n",
        "  model.to(device)\n",
        "\n",
        "  # Define loss function and optimizer\n",
        "  weight_for_0 = sum(class_counts.values()) / (class_counts[0] *  2.0) # total_samples / (num_samples_in_class_i * num_classes)\n",
        "  weight_for_1 = sum(class_counts.values()) / (class_counts[1] *  2.0)\n",
        "  weight = torch.tensor([weight_for_0, weight_for_1]).to(device)\n",
        "  criterion = nn.BCEWithLogitsLoss(weight=weight)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate) # can try SGD\n",
        "\n",
        "  # Train model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    avg_loss = running_loss/len(train_loader.sampler) # average loss over the epoch\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
        "\n",
        "  # Validation\n",
        "  y_pred_probas, y_pred, y_true = validate(model, val_loader, criterion, device)\n",
        "  accuracy, precision, recall, f1, specificity, roc_auc, pr_auc = calculate_evaluation_metrics(y_true, y_pred, y_pred_probas)\n",
        "\n",
        "  fold_accuracies.append(accuracy)\n",
        "  fold_precisions.append(precision)\n",
        "  fold_recalls.append(recall)\n",
        "  fold_specificities.append(specificity)\n",
        "  fold_f1_scores.append(f1)\n",
        "  fold_roc_auc.append(roc_auc)\n",
        "  fold_pr_auc.append(pr_auc)\n",
        "\n",
        "  print(f\"Validation Accuracy (Fold {fold + 1}/{k}): {accuracy:.2f}\")\n",
        "  print(f\"Precision (Fold {fold + 1}/{k}): {precision:.2f}\")\n",
        "  print(f\"Recall (Fold {fold + 1}/{k}): {recall:.2f}\")\n",
        "  print(f\"Specificity (Fold {fold + 1}/{k}): {specificity:.2f}\")\n",
        "  print(f\"F1 Score (Fold {fold + 1}/{k}): {f1:.2f}\")\n",
        "  print(f\"ROC AUC (Fold {fold + 1}/{k}): {roc_auc:.2f}\")\n",
        "  print(f\"PR AUC (Fold {fold + 1}/{k}): {pr_auc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOQou3IVPQ5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a913f2ed-5aff-46d6-c4a7-9e54a7a71bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.80\n",
            "Mean Precision: 0.84\n",
            "Mean Recall: 0.86\n",
            "Mean Specificity: 0.67\n",
            "Mean F1 Score: 0.85\n",
            "Mean ROC AUC: 0.86\n",
            "Mean PR AUC: 0.90\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print the mean accuracies across folds\n",
        "print_validation_performance(fold_accuracies, fold_precisions, fold_recalls, fold_specificities, fold_f1_scores, fold_roc_auc, fold_pr_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWPALofBWLDw"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNP47eOoVvqG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzINzVfAWNO5"
      },
      "outputs": [],
      "source": [
        "# to update best parameters possible for each metric\n",
        "def update_best_params(metric, best_params, score, epochs, batch_size, lr):\n",
        "  best_params[metric]['score'] = score\n",
        "  best_params[metric]['num_epochs'] = epochs\n",
        "  best_params[metric]['batch_size'] = batch_size\n",
        "  best_params[metric]['learning_rate'] = lr\n",
        "  return best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5hx1VNXV3YW"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(dataset, k, epoch_list, batch_list, lr_list, device):\n",
        "  \"\"\"\n",
        "  Tune hyperparemeters for Resnet model.\n",
        "\n",
        "  Parameters:\n",
        "  - dataset (List): The list of tuples containing the training data, where each tuple is (PIL image, label).\n",
        "  - k (int): Number of folds to be used for cross validation\n",
        "  - epoch_list (List): The list of number of epochs to be explored for hyperparameter tuning.\n",
        "  - batch_list (List): The list of batch sizes to be explored for hyperparameter tuning.\n",
        "  - lr_list (List): The list of learning rates to be explored for hyperparameter tuning.\n",
        "  - device (torch.device): The device (CPU or GPU) on which the validation will be performed.\n",
        "\n",
        "  Returns:\n",
        "  - List: A list of dictionaries, 'best_params' and 'params_result'.\n",
        "    - best_params (Dict): contains the best parameters for each metric\n",
        "    - params_result (Dict): contains all evaluation metrics results for all combinations of hyperparameters\n",
        "  \"\"\"\n",
        "  params_result = {'hyperparameters': [], # (epochs, batch_size, learning_rate)\n",
        "                   'accuracy': [],\n",
        "                   'precision': [],\n",
        "                   'recall': [],\n",
        "                   'specificity': [],\n",
        "                   'f1': [],\n",
        "                   'roc_auc': [],\n",
        "                   'pr_auc': []}\n",
        "\n",
        "  best_params = {'accuracy': {'score': float('-inf')},\n",
        "                 'precision': {'score': float('-inf')},\n",
        "                 'recall': {'score': float('-inf')},\n",
        "                 'specificity': {'score': float('-inf')},\n",
        "                 'f1': {'score': float('-inf')},\n",
        "                 'roc_auc': {'score': float('-inf')},\n",
        "                 'pr_auc': {'score': float('-inf')}}\n",
        "\n",
        "  data = [item[0] for item in dataset]\n",
        "  data_labels = [item[1] for item in dataset]\n",
        "  skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "  augmentation = Augmentation()\n",
        "\n",
        "  for batch_size in batch_list:\n",
        "    for num_epochs in epoch_list:\n",
        "      for learning_rate in lr_list:\n",
        "        print(f\"Trying hyperparameters: {num_epochs} epochs, {batch_size} batch_size, {learning_rate} learning rate\")\n",
        "\n",
        "        fold_accuracies, fold_precisions, fold_recalls, fold_specificities, fold_f1_scores = [], [], [], [], []\n",
        "        fold_roc_auc, fold_pr_auc = [], []\n",
        "\n",
        "        for fold, (train_index, val_index) in enumerate(skf.split(data, data_labels)):\n",
        "          # print(f\"Training Fold {fold + 1}/{k}\")\n",
        "\n",
        "          # initialize lists to store metrics for each fold\n",
        "          train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
        "          val_dataset = torch.utils.data.Subset(dataset, val_index)\n",
        "\n",
        "          class_counts = dict(Counter(dataset[i][1] for i in train_index))\n",
        "\n",
        "          augmented_images = []\n",
        "          for img_label in train_dataset:\n",
        "            augmented_images.extend(augmentation(img_label))\n",
        "\n",
        "          transformed_val_dataset = []\n",
        "          for img_label in val_dataset:\n",
        "              img, label = img_label\n",
        "              transformed_img = validation_transform(img)\n",
        "              transformed_val_dataset.append((transformed_img, label))\n",
        "\n",
        "          # create data loaders for training and validation\n",
        "          train_loader = DataLoader(augmented_images, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "          val_loader = DataLoader(transformed_val_dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "          # initialize the model\n",
        "          model = CNN()\n",
        "          model.to(device)\n",
        "\n",
        "          # define loss function and optimizer\n",
        "          weight_for_0 = sum(class_counts.values()) / (class_counts[0] *  2.0) # total_samples / (num_samples_in_class_i * num_classes)\n",
        "          weight_for_1 = sum(class_counts.values()) / (class_counts[1] *  2.0)\n",
        "          weight = torch.tensor([weight_for_0, weight_for_1]).to(device)\n",
        "          criterion = nn.BCEWithLogitsLoss(weight=weight)\n",
        "          optimizer = optim.Adam(model.parameters(), lr=learning_rate) # can try SGD\n",
        "\n",
        "          # train model\n",
        "          for epoch in range(num_epochs):\n",
        "            running_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            avg_loss = running_loss/len(train_loader.sampler) # average loss over the epoch\n",
        "            # print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
        "\n",
        "          # validation\n",
        "          y_pred_probas, y_pred, y_true = validate(model, val_loader, criterion, device)\n",
        "          accuracy, precision, recall, f1, specificity, roc_auc, pr_auc = calculate_evaluation_metrics(y_true, y_pred, y_pred_probas)\n",
        "\n",
        "          fold_accuracies.append(accuracy)\n",
        "          fold_precisions.append(precision)\n",
        "          fold_recalls.append(recall)\n",
        "          fold_specificities.append(specificity)\n",
        "          fold_f1_scores.append(f1)\n",
        "          fold_roc_auc.append(roc_auc)\n",
        "          fold_pr_auc.append(pr_auc)\n",
        "\n",
        "        # calculate and store the mean metrics for this hyperparameter combination\n",
        "        mean_accuracy = np.mean(fold_accuracies)\n",
        "        mean_precision = np.mean(fold_precisions)\n",
        "        mean_recall = np.mean(fold_recalls)\n",
        "        mean_specificity = np.mean(fold_specificities)\n",
        "        mean_f1_score = np.mean(fold_f1_scores)\n",
        "        mean_roc_auc = np.mean(fold_roc_auc)\n",
        "        mean_pr_auc = np.mean(fold_pr_auc)\n",
        "\n",
        "        params_result['hyperparameters'].append((num_epochs, batch_size, learning_rate))\n",
        "        params_result['accuracy'].append(mean_accuracy)\n",
        "        params_result['precision'].append(mean_precision)\n",
        "        params_result['recall'].append(mean_recall)\n",
        "        params_result['specificity'].append(mean_specificity)\n",
        "        params_result['f1'].append(mean_f1_score)\n",
        "        params_result['roc_auc'].append(mean_roc_auc)\n",
        "        params_result['pr_auc'].append(mean_pr_auc)\n",
        "\n",
        "        print(f\"Accuracy: {mean_accuracy:.2f}\")\n",
        "        print(f\"Precision: {mean_precision:.2f}\")\n",
        "        print(f\"Recall: {mean_recall:.2f}\")\n",
        "        print(f\"Specificity: {mean_specificity:.2f}\")\n",
        "        print(f\"F1 Score: {mean_f1_score:.2f}\")\n",
        "        print(f\"ROC-AUC: {mean_roc_auc:.2f}\")\n",
        "        print(f\"PR-AUC: {mean_pr_auc:.2f}\")\n",
        "        print('----------------------------------------------------------------')\n",
        "\n",
        "        # update best params dict\n",
        "        if mean_accuracy > best_params['accuracy']['score']:\n",
        "          best_params = update_best_params('accuracy', best_params, mean_accuracy, num_epochs, batch_size, learning_rate)\n",
        "        if mean_precision > best_params['precision']['score']:\n",
        "          best_params = update_best_params('precision', best_params, mean_precision, num_epochs, batch_size, learning_rate)\n",
        "        if mean_recall > best_params['recall']['score']:\n",
        "          best_params = update_best_params('recall', best_params, mean_recall, num_epochs, batch_size, learning_rate)\n",
        "        if mean_specificity > best_params['specificity']['score']:\n",
        "          best_params = update_best_params('specificity', best_params, mean_specificity, num_epochs, batch_size, learning_rate)\n",
        "        if mean_f1_score > best_params['f1']['score']:\n",
        "          best_params = update_best_params('f1', best_params, mean_f1_score, num_epochs, batch_size, learning_rate)\n",
        "        if mean_roc_auc > best_params['roc_auc']['score']:\n",
        "          best_params = update_best_params('roc_auc', best_params, mean_roc_auc, num_epochs, batch_size, learning_rate)\n",
        "        if mean_pr_auc > best_params['pr_auc']['score']:\n",
        "          best_params = update_best_params('pr_auc', best_params, mean_pr_auc, num_epochs, batch_size, learning_rate)\n",
        "\n",
        "  print(f\"Best hyperparameters: {best_params}\")\n",
        "\n",
        "  return best_params, params_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufG90Ud-WDf1",
        "outputId": "f1698b3e-8ecd-4dfc-d7b8-c271033cd4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "184\n",
            "46\n"
          ]
        }
      ],
      "source": [
        "# Split dataset to 80% train, 20% test\n",
        "y = [item[1] for item in dataset.samples]\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, shuffle=True, stratify=y)\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlsXUMjWWGZd",
        "outputId": "9d5b46ee-adb1-4cf3-e733-20a0dd879401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Trying hyperparameters: 10 epochs, 32 batch_size, 0.001 learning rate\n",
            "Accuracy: 0.76\n",
            "Precision: 0.85\n",
            "Recall: 0.78\n",
            "Specificity: 0.71\n",
            "F1 Score: 0.81\n",
            "ROC-AUC: 0.79\n",
            "PR-AUC: 0.86\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 10 epochs, 32 batch_size, 0.01 learning rate\n",
            "Accuracy: 0.76\n",
            "Precision: 0.81\n",
            "Recall: 0.83\n",
            "Specificity: 0.63\n",
            "F1 Score: 0.82\n",
            "ROC-AUC: 0.76\n",
            "PR-AUC: 0.83\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 20 epochs, 32 batch_size, 0.001 learning rate\n",
            "Accuracy: 0.77\n",
            "Precision: 0.83\n",
            "Recall: 0.83\n",
            "Specificity: 0.66\n",
            "F1 Score: 0.83\n",
            "ROC-AUC: 0.81\n",
            "PR-AUC: 0.88\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 20 epochs, 32 batch_size, 0.01 learning rate\n",
            "Accuracy: 0.74\n",
            "Precision: 0.83\n",
            "Recall: 0.77\n",
            "Specificity: 0.68\n",
            "F1 Score: 0.80\n",
            "ROC-AUC: 0.80\n",
            "PR-AUC: 0.88\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 50 epochs, 32 batch_size, 0.001 learning rate\n",
            "Accuracy: 0.78\n",
            "Precision: 0.82\n",
            "Recall: 0.86\n",
            "Specificity: 0.61\n",
            "F1 Score: 0.84\n",
            "ROC-AUC: 0.79\n",
            "PR-AUC: 0.87\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 50 epochs, 32 batch_size, 0.01 learning rate\n",
            "Accuracy: 0.72\n",
            "Precision: 0.82\n",
            "Recall: 0.75\n",
            "Specificity: 0.66\n",
            "F1 Score: 0.78\n",
            "ROC-AUC: 0.74\n",
            "PR-AUC: 0.84\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 10 epochs, 64 batch_size, 0.001 learning rate\n",
            "Accuracy: 0.76\n",
            "Precision: 0.80\n",
            "Recall: 0.87\n",
            "Specificity: 0.54\n",
            "F1 Score: 0.83\n",
            "ROC-AUC: 0.79\n",
            "PR-AUC: 0.83\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 10 epochs, 64 batch_size, 0.01 learning rate\n",
            "Accuracy: 0.71\n",
            "Precision: 0.80\n",
            "Recall: 0.75\n",
            "Specificity: 0.63\n",
            "F1 Score: 0.78\n",
            "ROC-AUC: 0.72\n",
            "PR-AUC: 0.81\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 20 epochs, 64 batch_size, 0.001 learning rate\n",
            "Accuracy: 0.78\n",
            "Precision: 0.85\n",
            "Recall: 0.82\n",
            "Specificity: 0.71\n",
            "F1 Score: 0.83\n",
            "ROC-AUC: 0.79\n",
            "PR-AUC: 0.84\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 20 epochs, 64 batch_size, 0.01 learning rate\n",
            "Accuracy: 0.71\n",
            "Precision: 0.85\n",
            "Recall: 0.67\n",
            "Specificity: 0.78\n",
            "F1 Score: 0.75\n",
            "ROC-AUC: 0.78\n",
            "PR-AUC: 0.82\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 50 epochs, 64 batch_size, 0.001 learning rate\n",
            "Accuracy: 0.79\n",
            "Precision: 0.85\n",
            "Recall: 0.84\n",
            "Specificity: 0.69\n",
            "F1 Score: 0.84\n",
            "ROC-AUC: 0.82\n",
            "PR-AUC: 0.88\n",
            "----------------------------------------------------------------\n",
            "Trying hyperparameters: 50 epochs, 64 batch_size, 0.01 learning rate\n",
            "Accuracy: 0.77\n",
            "Precision: 0.83\n",
            "Recall: 0.82\n",
            "Specificity: 0.66\n",
            "F1 Score: 0.82\n",
            "ROC-AUC: 0.78\n",
            "PR-AUC: 0.86\n",
            "----------------------------------------------------------------\n",
            "Best hyperparameters: {'accuracy': {'score': 0.788138138138138, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001}, 'precision': {'score': 0.8529523809523809, 'num_epochs': 20, 'batch_size': 64, 'learning_rate': 0.001}, 'recall': {'score': 0.8686666666666666, 'num_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001}, 'specificity': {'score': 0.7756410256410255, 'num_epochs': 20, 'batch_size': 64, 'learning_rate': 0.01}, 'f1': {'score': 0.8406944368866881, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001}, 'roc_auc': {'score': 0.8202136752136753, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001}, 'pr_auc': {'score': 0.8796929131155805, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001}}\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters for tuning\n",
        "epoch_list = [10, 20, 50]\n",
        "batch_list = [32, 64]\n",
        "lr_list = [0.001, 0.01]\n",
        "k = 5\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "best_params, params_result = tune_hyperparameters(train_dataset, k, epoch_list, batch_list, lr_list, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best hyperparameters:\n",
        "- 'accuracy': {'score': 0.788138138138138, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001},\n",
        "- 'precision': {'score': 0.8529523809523809, 'num_epochs': 20, 'batch_size': 64, 'learning_rate': 0.001},\n",
        "- 'recall': {'score': 0.8686666666666666, 'num_epochs': 10, 'batch_size': 64, 'learning_rate': 0.001},\n",
        "- 'specificity': {'score': 0.7756410256410255, 'num_epochs': 20, 'batch_size': 64, 'learning_rate': 0.01},\n",
        "- 'f1': {'score': 0.8406944368866881, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001},\n",
        "- 'roc_auc': {'score': 0.8202136752136753, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001},\n",
        "- 'pr_auc': {'score': 0.8796929131155805, 'num_epochs': 50, 'batch_size': 64, 'learning_rate': 0.001}}"
      ],
      "metadata": {
        "id": "QDEe3P0CGJuu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN0hwuqNWxnK"
      },
      "source": [
        "### Training on Tuned Hyperparameters\n",
        "- we picked the parameters that gave the best `pr_auc`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEv9kpf2W0Gt",
        "outputId": "5be78967-9b69-40a1-8223-7a379ed2ec0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.982341118480848\n",
            "Epoch 2/50, Loss: 0.6983329721119093\n",
            "Epoch 3/50, Loss: 0.5292163009228914\n",
            "Epoch 4/50, Loss: 0.39814657024715255\n",
            "Epoch 5/50, Loss: 0.3317219508730847\n",
            "Epoch 6/50, Loss: 0.43021017157513164\n",
            "Epoch 7/50, Loss: 0.22582132919974948\n",
            "Epoch 8/50, Loss: 0.2997489799623904\n",
            "Epoch 9/50, Loss: 0.18272083200838254\n",
            "Epoch 10/50, Loss: 0.15409185951170715\n",
            "Epoch 11/50, Loss: 0.1149463133967441\n",
            "Epoch 12/50, Loss: 0.08573109961074332\n",
            "Epoch 13/50, Loss: 0.06917280619559081\n",
            "Epoch 14/50, Loss: 0.09758652565271958\n",
            "Epoch 15/50, Loss: 0.07112959509958391\n",
            "Epoch 16/50, Loss: 0.049172319633805237\n",
            "Epoch 17/50, Loss: 0.03209486014169195\n",
            "Epoch 18/50, Loss: 0.019186994805932046\n",
            "Epoch 19/50, Loss: 0.01673772223293781\n",
            "Epoch 20/50, Loss: 0.011437523956208125\n",
            "Epoch 21/50, Loss: 0.008942639981598957\n",
            "Epoch 22/50, Loss: 0.011053466246179913\n",
            "Epoch 23/50, Loss: 0.0073679758559750475\n",
            "Epoch 24/50, Loss: 0.006222996423425882\n",
            "Epoch 25/50, Loss: 0.005684048714845077\n",
            "Epoch 26/50, Loss: 0.005658458618690138\n",
            "Epoch 27/50, Loss: 0.004567986491905606\n",
            "Epoch 28/50, Loss: 0.004476702792327042\n",
            "Epoch 29/50, Loss: 0.005245367964000805\n",
            "Epoch 30/50, Loss: 0.004518217435511558\n",
            "Epoch 31/50, Loss: 0.004073808605418257\n",
            "Epoch 32/50, Loss: 0.0031242914578836897\n",
            "Epoch 33/50, Loss: 0.0027360049076378345\n",
            "Epoch 34/50, Loss: 0.00255305432593045\n",
            "Epoch 35/50, Loss: 0.0028844932580123776\n",
            "Epoch 36/50, Loss: 0.0025799686939496063\n",
            "Epoch 37/50, Loss: 0.0024824157741892595\n",
            "Epoch 38/50, Loss: 0.002224559093946996\n",
            "Epoch 39/50, Loss: 0.002399882694463367\n",
            "Epoch 40/50, Loss: 0.0017876273648732381\n",
            "Epoch 41/50, Loss: 0.0018715669016313293\n",
            "Epoch 42/50, Loss: 0.001969753428483787\n",
            "Epoch 43/50, Loss: 0.0018492604170561484\n",
            "Epoch 44/50, Loss: 0.0016751901853991591\n",
            "Epoch 45/50, Loss: 0.0016262181953567525\n",
            "Epoch 46/50, Loss: 0.0014179991882132448\n",
            "Epoch 47/50, Loss: 0.0011962507385760545\n",
            "Epoch 48/50, Loss: 0.0015010476800734582\n",
            "Epoch 49/50, Loss: 0.01023576798160439\n",
            "Epoch 50/50, Loss: 0.17958735969403516\n",
            "Test Accuracy: 0.85\n",
            "Test Precision: 0.90\n",
            "Test Recall: 0.87\n",
            "Test Specificity: 0.80\n",
            "Test F1 Score: 0.89\n",
            "Test ROC-AUC: 0.82\n",
            "Test PR-AUC: 0.89\n"
          ]
        }
      ],
      "source": [
        "# best hyperparameters for pr_auc\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "augmentation = Augmentation()\n",
        "\n",
        "class_counts = dict(Counter(train_dataset[i][1] for i in range(len(train_dataset))))\n",
        "\n",
        "# use train_dataset and test_dataset split during hyperparameter tuning\n",
        "augmented_images = []\n",
        "for img_label in train_dataset:\n",
        "  augmented_images.extend(augmentation(img_label))\n",
        "\n",
        "transformed_test_dataset = []\n",
        "for img_label in test_dataset:\n",
        "    img, label = img_label\n",
        "    transformed_img = validation_transform(img)\n",
        "    transformed_test_dataset.append((transformed_img, label))\n",
        "\n",
        "train_loader = DataLoader(augmented_images, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(transformed_test_dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "# initialize the model\n",
        "model = CNN()\n",
        "model.to(device)\n",
        "\n",
        "# define loss function and optimizer\n",
        "weight_for_0 = sum(class_counts.values()) / (class_counts[0] *  2.0) # total_samples / (num_samples_in_class_i * num_classes)\n",
        "weight_for_1 = sum(class_counts.values()) / (class_counts[1] *  2.0)\n",
        "weight = torch.tensor([weight_for_0, weight_for_1]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(weight=weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # can try SGD\n",
        "\n",
        "# train model\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "  avg_loss = running_loss/len(train_loader.sampler) # average loss over the epoch\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
        "\n",
        "# test\n",
        "y_pred_probas, y_pred, y_true = validate(model, test_loader, criterion, device)\n",
        "accuracy, precision, recall, f1, specificity, roc_auc, pr_auc = calculate_evaluation_metrics(y_true, y_pred, y_pred_probas)\n",
        "print('----------------------------------------------------------------')\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Test Precision: {precision:.2f}\")\n",
        "print(f\"Test Recall: {recall:.2f}\")\n",
        "print(f\"Test Specificity: {specificity:.2f}\")\n",
        "print(f\"Test F1 Score: {f1:.2f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc:.2f}\")\n",
        "print(f\"Test PR-AUC: {pr_auc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Test Accuracy: 0.85\n",
        "- Test Precision: 0.90\n",
        "- Test Recall: 0.87\n",
        "- Test Specificity: 0.80\n",
        "- Test F1 Score: 0.89\n",
        "- Test ROC-AUC: 0.82\n",
        "- Test PR-AUC: 0.89"
      ],
      "metadata": {
        "id": "RwBYLvUq-BMr"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}